# Supervised Fine-Tuning on the Hub

- ğŸ¢ 75 XP â€” LoRA adapter fine-tuned on a small instruction set with the adapter artifact uploaded to the Hub.
- ğŸ• 125 XP â€” Direct Preference Optimization (DPO) run on <=5k preference pairs with the adapter and reward curves published alongside the model card.
- ğŸ¦ 225 XP â€” Group Relative Policy Optimization (GRPO) run with a custom reward function, monitor stability, and publish policy + reward logs.

